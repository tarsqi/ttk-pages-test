<link rel="stylesheet" type="text/css" href="master.css">
</head>

<body>

<p class="top navigation">
<a href="index.html">home</a>
<a href="1-toplevel.html">toplevel</a>
<a href="2-preprocessor.html">preprocessor</a>
<a href="3-gutime.html">gutime</a>
<a href="4-evita.html">evita</a>
<a href="5-slinket.html">slinket</a>
<a href="6-s2t.html">s2t</a>
<a href="7-blinker.html">blinker</a>
<a href="8-classifier.html">classifier</a>
<a href="9-sputlink.html">sputlink</a>
</p>


<h2>TARSQI Toolkit - The Preprocessor</h2>

<p>The standard Tarsqi preprocessor can be run in two ways:</p>

<pre class="example indent">
$ python tarsqi.py --pipeline=PREPROCESSOR &lt;INFILE> &lt;OUTFILE>
$ python tarsqi.py --pipeline=TOKENIZER,TAGGER,CHUNKER &lt;INFILE> &lt;OUTFILE>
</pre>

<p>In the first case, the PreprocessorWrapper is used, which wraps tokenizer,
tagger and chunker and chunker all together. In the second case, the tokenizer,
tagger and chunker are each wrapped individually. The second invocation allows
some more flexibility. For example, we can now run TTK on input that was already
tokenized, but not tagged and chunked. This is useful in case we want to adopt
external tokenization. In addition, it is now easier to swap in different
taggers if needed. We would just an additional tagger component and reference
that one in the pipeline (some extra coding would be needed to write the wrapper
of course). The default is to use the PREPROCESSOR.</p>
  
  
<h3>PreProcessorWrapper</h3>

<p>The PreprocessorWrapper loops through the document elements by using the
TarsqiDocument.elements() method, which returns a list of Tags of type
"docelement" where each tag has pointers to the begin and end offset in the
text. For each element, the wrapper extracts the source text, runs the
tokenizer, tagger and chunker on that text and then exports the results back to
the TarsqiDocElements.</p>

<img src="images/preprocessing.png" height="350"/>

<p>The tokenizer gets a copy of a slice of the text from the TarsqiDocument
(stored in the SourceDoc in the source instance variable) and it returns a list
of pairs, where each pair is either ('&lt;s>', None) for sentence boundaries or
a pair of a string and a TokenizedLex instance, which has instance variables
begin, end and text:</p>

<pre class="example indent">
[('&lt;s>', None),
 (u'Fido', &lt;components.preprocessing.tokenizer.TokenizedLex instance at 0x110844998>),
 (u'barks', &lt;components.preprocessing.tokenizer.TokenizedLex instance at 0x110844ab8>),
 (u'.', &lt;components.preprocessing.tokenizer.TokenizedLex instance at 0x110844b48>)]
</pre>

<p>Since the tokenizer runs in isolation on the text of an element, it assigns
offsets starting at 0. The PreprocessorWrapper adjusts these so that the offsets
point into the correct spot in the full text source of the document.</p>

<p>The tagger is then fed a vertical string consisting of the first element of
all pairs (the s tag or a string):</p>

<pre class="example indent">
&lt;s>
Fido
barks
.
</pre>

The tagger returns a list with as many elements as lines, where each element is
either an s tag or a tab-separated triple of string, part-of-speech tag and
lemma:</p>

<pre class="example indent">
['&lt;s>', 
 'Fido\tNP\tFido', 
 'barks\tVVZ\tbark', 
 '.\tSENT\t.']
</pre>

<p>The PreprocessorWrapper then takes this list and merges it with the list of
pairs that came out of the tokenizer and creates the following structure:</p>

<pre class="example indent">
[[('Fido', 'NNP', 'Fido', 1, 5),
  ('barks', 'VBZ', 'bark', 6, 11),
  ('.', '.', '.', 11, 12)]]
</pre>

<p>Note that the s tags have disappeared and that instead we now have a list of
sublists, with one sublist for each sentence. Another thing that happens at this
transformation stage is some normalization of tag names. The chunker adds ng and
vg tags to the sublists.</p>

<pre class="example indent">
[['&lt;ng>', ('Fido', 'NNP', 'Fido', 1, 5), '&lt;/ng>',
  '&lt;vg>', ('barks', 'VBZ', 'bark', 6, 11), '&lt;/vg>',
  ('.', '.', '.', 11, 12)]]
</pre>

<p>Finally, the information in this data structure is exported to the
TagRepository in the tags instance variable on the TarsqiDocument and with the
above input the preprocessor will append s, ng, vg and lex tags to the tags list
and eventually set it to:</p>

<pre class="example indent">
[ &lt;Tag docelement id=d1 0:13 {'type': paragraph'}>,
  &lt;Tag lex id=l1 1:5 {'lemma': 'Fido', 'pos': 'NNP'}>,
  &lt;Tag ng id=c1 1:5 {}>,
  &lt;Tag lex id=l2 6:11 {'lemma': 'bark', 'pos': 'VBZ'}>,
  &lt;Tag vg id=c2 6:11 {}>,
  &lt;Tag lex id=l3 11:12 {'lemma': '.', 'pos': '.'}>,
  &lt;Tag s id=s1 1:12 {}> ]
</pre>

<p>Notice that the tags are added by a depth-first post-order traversal of the
tree, but this is an accidental feature of the algorithm and in no way a
requirement on the order of the tags. When this tags list is built the
preprocessor uses the index() method on the TagRepository to create the
opening_tags and closing_tags dictionaries, which will look as follows:</p>

<pre class="example indent">
{ 0: [ &lt;Tag docelement id=d1 0:13 {'type': paragraph'}>,
  1: [ &lt;Tag s id=s1 1:12 {}>,
       &lt;Tag lex id=l1 1:5 {'lemma': 'Fido', 'pos': 'NNP'}>,
       &lt;Tag ng id=c1 1:5 {}> ],
  6: [ &lt;Tag lex id=l2 6:11 {'lemma': 'bark', 'pos': 'VBZ'}>,
       &lt;Tag vg id=c2 6:11 {}> ],
 11: [ &lt;Tag lex id=l3 11:12 {'lemma': '.', 'pos': '.'}> ] }
</pre>

<pre class="example indent">
{  5: { 1: {'lex': True, 'ng': True}}
  11: { 6: {'lex': True, 'vg': True}},
  12: { 1: {'s': True},
       11: {'lex': True}},
  13: { 0: {'docelement': True}}}
</pre>

<p>These dictionaries can be used for quick access based on character
offsets.</p>


<h3>TokenizerWrapper, TaggerWrapper and ChunkerWrapper</h3>

These work with the same tokenizer, tagger and chunker as the
PreprocessorWrapper and the tokenizer, tagger and chunker take the same input
and create the same output no matter what wrapper they are called from. The
difference is that each wrapper will retrieve the data it needs from the
TarsqiDocument and will always export to the TarsqiDocument. In contrast, with
the PreprocessorWrapper we could almost directly pipe the output of the tagger
into the chunker, without doing an export and import inbetween.

</body>
</html>
